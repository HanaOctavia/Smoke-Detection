# -*- coding: utf-8 -*-
"""38 FINAL ML Terapan-Perojek 1-Perdictive analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j7sWwIN34eDJxblaO1M1MfI4QDxXhNJ8

# Projek ML Terapan 1 : Mendeteksi asap rokok

### Hana Octavia Trinida Malo

###M05
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score

"""# Data Understanding

**1. Data Loading**
"""

dataset = '/content/smoke_detection_iot.csv'
sp = pd.read_csv(dataset)
sp

"""**2. Mengecek dan menangani missing value**"""

sp.isnull().sum()

"""Data kita tidak memiliki nilai null"""

sp.info()

# menghapus tabel 'Unnamed: 0' karena tidak memberikan pengaruh apapun 
sp = sp.drop(['Unnamed: 0'], axis = 1)
sp

# melihat kolom apa saja yang ada dan berapa banyak data
print('columns: ', list(sp.columns))
print('shape: ', sp.shape)

"""**3. Menangani Outliers**

Mengecek apakah ada outliners
"""

def diagnostic_plots(sp, variable) :
    plt.figure(figsize=(20, 4))
    plt.subplot(1, 4, 3)
    sns.boxplot(x=sp[variable],color = 'pink')
    plt.title('Boxplot')

    plt.show()

for variable in sp:
    diagnostic_plots(sp,variable)

"""dilihat dari hasil grafik diatas terdapat outliners

sehingga kita dapat menangani outliners dengan cara berikut

"""

Q1 = sp.quantile(0.25)
Q3 = sp.quantile(0.75)
IQR=Q3-Q1
sp=sp[~((sp<(Q1-1.5*IQR))|(sp>(Q3+1.5*IQR))).any(axis=1)]
 
# Cek ukuran dataset setelah kita drop outliers
sp.shape

"""Terlihat bahwa dataset kita hanya hanya memiliki 35684 data

Dataset Anda sekarang telah bersih dan memiliki 47.524 sampel.

**4. proses analisis data dengan teknik Univariate EDA.**

Membagi fitur pada dataset menjadi dua bagian, yaitu numerical features dan categorical features.
"""

numerical_features = ['UTC', 'Temperature[C]', 'Humidity[%]', 'TVOC[ppb]', 'eCO2[ppm]', 'Raw H2', 'Raw Ethanol', 'Pressure[hPa]', 'PM1.0', 'PM2.5', 'NC0.5', 'NC1.0', 'NC2.5', 'CNT']
categorical_features =  ['Fire Alarm']

"""Analisis terhadap fitur kategori"""

# Fitur Fire Alarm
feature = categorical_features[0]
count = sp[feature].value_counts()
percent = 100*sp[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""c. Numerical Features

melihat histogram masing-masing fiturnya
"""

sp.hist(bins=50, figsize=(20,15), color='pink')
plt.show()

"""**5. EDA-Multivariate Analysis**"""

plt.figure(figsize=(10, 8))
correlation_matrix = sp.corr().round(2)
 
sns.heatmap(data=correlation_matrix, annot=True, cmap='Purples', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Dari visualisasi di atas ada beberapa kesimpulan


*   variabel CNT memiliki korelasi yang paling tinggi dengan Fire Alarm, namun sayangnya data yang dimiliki tidak berpengaruh pada Fire alarm karena hanya merupakan data Simple count


*   variabel TVOC[ppb] memiliki korelasi yang paling tinggi dengan variabele fire alarm



*   variabel Temperature, humidity, eCO2 memiliki korelasi yang sangat rendah dengan data target kita, sehingga kita bisa mengabaikan variabel ini dengan cara mendropnya


"""

# drop variabel-variabel yang berkorelasi rendah dengan 'Fire Alarm'
sp.drop(['UTC', 'CNT', 'Humidity[%]', 'Temperature[C]', 'eCO2[ppm]'], inplace=True, axis=1)
sp.head()

"""# Data Preparation

**6. Train-Test-Split**

Membagi data menjadi data train dan data set dengan data test sebesar 20%
"""

from sklearn.model_selection import train_test_split
 
X = sp.drop(["Fire Alarm"],axis =1)
y = sp["Fire Alarm"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

"""mengecek jumlah sampel pada data train dan data test"""

print(f'Jumlah Seluruh Dataset: {len(X)}')
print(f'Jumlah data train: {len(X_train)}')
print(f'Jumlah data test: {len(X_test)}')

"""**7. Standarisasi ***"""

from sklearn.preprocessing import StandardScaler
 
numerical_features = ['TVOC[ppb]', 'Raw H2', 'Raw Ethanol', 'Pressure[hPa]', 'PM1.0', 'PM2.5', 'NC0.5', 'NC1.0', 'NC2.5']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""mengecek nilai mean dan standar deviasi pada setelah proses standarisasi"""

X_train[numerical_features].describe().round(4)

"""# Model Development

**8. Menggunakan Bernoulli Naive Bayes**
"""

from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import mean_squared_error

bnb = BernoulliNB()
bnb.fit(X_train, y_train)

"""**9. SVM**"""

from sklearn.svm import SVC
svc = SVC(random_state = 42)
svc.fit(X_train, y_train)

"""**10. Logistic Regressions**"""

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression(solver='lbfgs', max_iter=1000)
LR.fit(X_train, y_train)

"""# Evaluasi Model

**11. Scalling**
"""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""**12. Melihat akurasi model**"""

# membuat variabel untuk menampung model
models = [
    ("bnb" , bnb),
    ("svc" , svc),
    ("LR", LR)
]

for nama_model, model in models:
    # Prediksi y_pred
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)

    # melihat akurasi model
    print('{:s} acc score : {:.3f}'.format(nama_model, acc))

"""**13. Melihat f1 score**"""

for nama_model, model in models:
    # Prediksi y_pred
    y_pred = model.predict(X_test)
    f1 = f1_score(y_test, y_pred)

    # melihat akurasi model
    print('{:s} f1 score : {:.3f}'.format(nama_model, f1))

# Buat variabel evl 
evl = pd.DataFrame(columns=['acc', 'f1'], index=['bnb','svc','LR'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {"bnb" : bnb, "svc" : svc, "LR" : LR}
y_pred = model.predict(X_test)

for name, model in model_dict.items():
    evl.loc[name, 'acc'] = accuracy_score(y_test, y_pred)
    evl.loc[name, 'f1'] = f1_score(y_test, y_pred)
 

fig, ax = plt.subplots()
evl.sort_values(by = 'acc', ascending=False).plot(kind='barh', ax=ax, zorder=5)
ax.grid(zorder=0)

"""**14. Melakukan prediksi**"""

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'bnn': bnb, 'SVM': svc, 'LR': LR}

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)